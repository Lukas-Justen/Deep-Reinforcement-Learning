{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning  - Project\n",
    "### Algorithm Efficiency Exploration\n",
    "\n",
    "##### Idea\n",
    "In this project, we would like to compare how different modern and potentially traditional control algorithms perform on one or more of the open AI gym environments. At the start, we hope to efficiently implement “traditional” deep Q-learning on one of the Atari environments. This is our core task. Then, depending on the complexity of this task, we plan to either attempt the same algorithm on a separate environment, or try additional algorithms on the same environment. From there, we hope to compare performance and training details for our different control models ( how quickly was performance achieved, were there substantial differences in compute needed, etc.). We think this is a very valuable project as it will let us explore implementation of the “core” deep reinforcement learning algorithm, and then dive into alternatives based on our progress. \n",
    "\n",
    "Link to Environments https://gym.openai.com/envs/#atari\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = gym.make(\"CartPole-v1\")\n",
    "EPISODES = 5000\n",
    "STEPS = 1000\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "OBSERVATION_SIZE = 4\n",
    "ACTION_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment that we are using has two discrete actions (move cart left, move cart right). Each action setp results in an observation which consists of 4 continous values that can range from a lower to an upper bound. My understanding here is that we cannot create a Q-Matrix because there is no way we can represent all these observations/states in a single 2 or 3 dimensional matrix. Therefore, we probably need to train a neural network or any other kind of machine learning component, that takes the observation into account to calculate the Q value for each of the two actions. During training time we expect that our neural network will converge to the real Q matrix. All in all, our neural network will take a single input (observation) and create multiple outputs (Q value for each action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "Discrete(2)\n",
      "\n",
      "Observation Space:\n",
      "Box(4,)\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\")\n",
    "print(ENVIRONMENT.action_space)\n",
    "\n",
    "print(\"\\nObservation Space:\")\n",
    "print(ENVIRONMENT.observation_space)\n",
    "print(ENVIRONMENT.observation_space.low)\n",
    "print(ENVIRONMENT.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/ReinforcementLearning/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Neural Net for Deep Q Learning\n",
    "# Sequential() creates the foundation of the layers.\n",
    "model = Sequential()\n",
    "# Input Layer of state size(4) and Hidden Layer with 24 nodes\n",
    "model.add(Dense(24, input_dim=OBSERVATION_SIZE, activation='relu'))\n",
    "# Hidden layer with 24 nodes\n",
    "model.add(Dense(24, activation='relu'))\n",
    "# Output Layer with # of actions: 2 nodes (left, right)\n",
    "model.add(Dense(ACTION_SIZE, activation='linear'))\n",
    "# Create the model based on the information above\n",
    "model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000 0.38\n",
      "WARNING:tensorflow:From /anaconda3/envs/ReinforcementLearning/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "50 5000 11.2\n",
      "100 5000 8.29\n",
      "150 5000 15.1\n",
      "200 5000 41.81\n",
      "250 5000 39.59\n",
      "300 5000 42.41\n",
      "350 5000 47.3\n",
      "400 5000 48.06\n",
      "450 5000 45.29\n",
      "500 5000 84.54\n",
      "550 5000 136.47\n",
      "600 5000 143.16\n",
      "650 5000 185.4\n",
      "700 5000 186.09\n",
      "750 5000 166.07\n",
      "800 5000 207.65\n",
      "850 5000 242.67\n",
      "900 5000 207.08\n",
      "950 5000 226.39\n",
      "1000 5000 245.12\n",
      "1050 5000 63.83\n",
      "1100 5000 176.93\n",
      "1150 5000 68.18\n",
      "1200 5000 179.24\n",
      "1250 5000 225.32\n",
      "1300 5000 236.31\n",
      "1350 5000 230.79\n",
      "1400 5000 250.0\n",
      "1450 5000 191.41\n",
      "1500 5000 234.7\n",
      "1550 5000 199.68\n",
      "1600 5000 224.63\n",
      "1650 5000 184.9\n",
      "1700 5000 225.86\n",
      "1750 5000 242.38\n",
      "1800 5000 245.11\n",
      "1850 5000 239.67\n",
      "1900 5000 197.28\n",
      "1950 5000 94.39\n",
      "2000 5000 192.06\n",
      "2050 5000 232.14\n",
      "2100 5000 147.17\n",
      "2150 5000 197.06\n",
      "2200 5000 165.56\n",
      "2250 5000 209.38\n",
      "2300 5000 180.0\n",
      "2350 5000 99.23\n",
      "2400 5000 114.48\n",
      "2450 5000 159.3\n",
      "2500 5000 193.6\n",
      "2550 5000 60.04\n",
      "2600 5000 80.41\n",
      "2650 5000 67.67\n",
      "2700 5000 40.96\n",
      "2750 5000 63.26\n",
      "2800 5000 78.01\n",
      "2850 5000 97.53\n",
      "2900 5000 107.32\n",
      "2950 5000 91.92\n",
      "3000 5000 130.97\n",
      "3050 5000 99.63\n",
      "3100 5000 110.65\n",
      "3150 5000 63.06\n",
      "3200 5000 58.78\n",
      "3250 5000 65.46\n",
      "3300 5000 69.62\n",
      "3350 5000 111.27\n",
      "3400 5000 152.57\n",
      "3450 5000 153.92\n",
      "3500 5000 52.79\n",
      "3550 5000 90.04\n",
      "3600 5000 79.73\n",
      "3650 5000 118.64\n",
      "3700 5000 117.48\n",
      "3750 5000 81.69\n",
      "3800 5000 105.03\n",
      "3850 5000 41.88\n",
      "3900 5000 212.06\n",
      "3950 5000 198.75\n",
      "4000 5000 239.79\n",
      "4050 5000 184.11\n",
      "4100 5000 172.33\n",
      "4150 5000 216.19\n",
      "4200 5000 238.51\n",
      "4250 5000 206.25\n",
      "4300 5000 136.55\n",
      "4350 5000 162.3\n",
      "4400 5000 112.77\n",
      "4450 5000 117.48\n",
      "4500 5000 126.89\n",
      "4550 5000 125.61\n",
      "4600 5000 192.58\n",
      "4650 5000 114.44\n",
      "4700 5000 188.99\n",
      "4750 5000 67.61\n",
      "4800 5000 71.91\n",
      "4850 5000 110.23\n",
      "4900 5000 89.37\n",
      "4950 5000 86.19\n"
     ]
    }
   ],
   "source": [
    "# Setup the memory for training the neural network\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "average_steps = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    # Prepare everything for a new episode\n",
    "    observation = ENVIRONMENT.reset()\n",
    "    observation = np.reshape(observation, (1,4))\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while t < STEPS and not done:\n",
    "        # Render the environment based on the update\n",
    "        ENVIRONMENT.render()\n",
    "        \n",
    "        # Take a new action either randomly or based on Q\n",
    "        r = np.random.rand(1)\n",
    "        if r < EPSILON:\n",
    "            action = ENVIRONMENT.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(model.predict(observation)[0])\n",
    "            \n",
    "        # Simulate the action based on the current state/observation\n",
    "        next_observation, reward, done, info = ENVIRONMENT.step(action)\n",
    "        next_observation = np.reshape(next_observation, (1,4))\n",
    "        \n",
    "        # Remember the last observations, actions and rewards\n",
    "        memory.append((observation, action, reward, next_observation, done))\n",
    "        \n",
    "        observation = next_observation\n",
    "        t += 1\n",
    "    \n",
    "    average_steps += t\n",
    "    if e % 50 == 0:\n",
    "        print(e,EPISODES, average_steps/100)\n",
    "        average_steps = 0\n",
    "                \n",
    "    if len(memory) >= BATCH_SIZE:\n",
    "        minibatch = random.sample(memory, BATCH_SIZE)\n",
    "        for observation, action, reward, next_observation, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                prediction = model.predict(next_observation)\n",
    "                target = reward + GAMMA * np.amax(prediction[0])\n",
    "            target_f = model.predict(observation)\n",
    "            target_f[0][action] = target\n",
    "            model.fit(observation, target_f, epochs=1, verbose=0)\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON = EPSILON * EPSILON_DECAY\n",
    "        \n",
    "# Close the environment\n",
    "ENVIRONMENT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model reconstruction from JSON file\n",
    "with open('model_architecture.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
